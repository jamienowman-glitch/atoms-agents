# Handover: AirCanvas Muscle Integration

## Status
*   **Core Logic**: Implemented in `service.py`. Validated via unit tests (`test_air_canvas.py`) with mocks.
*   **Automation**: `mcp.py` and `SKILL.md` successfully generated by Sentinel/Factory.
*   **Registry**: `sync_muscles.py` dry-run verified the muscle is detected correctly.

## Environment Limitations & Next Steps
The development environment lacked `ffmpeg` binaries and `atoms-core` Vault secrets. The following steps require a full environment:

1.  **Integration Test (Real Video)**:
    *   Run `service.py` with a real `.mp4` file containing a **Bright Orange** object (e.g., Tennis Ball, Orange Fruit, Post-it).
    *   **Default Color**: The code is now tuned for Orange (Hue 10-25).
    *   Ensure `ffmpeg` / `cv2.VideoWriter` output works correctly (audio/video muxing might be needed if audio preservation is required, currently uses `cv2` mp4v which is silent).
    *   **Verify Compositing**: Check if the "black" background of the canvas correctly becomes transparent over the video. The current logic uses `frame[mask] = canvas[mask]`, which is opaque. If blending is desired, switch to `cv2.addWeighted`.

2.  **Registry Sync**:
    *   Run `python3 scripts/sync_muscles.py` (from atoms-core) with valid Vault keys (`NORTHSTAR_KEYS_PATH`) to register the muscle in Supabase.

3.  **Parameter Tuning**:
    *   `gesture_threshold`: Set to 500 px/s by default. Test with real hand waves. The smoothing buffer (deque size 8) dampens velocity, so you may need to lower the threshold or reduce buffer size if gestures are hard to trigger.

## Run Verification
To verify the current state:
```bash
# Run unit tests
python3 src/muscle/video/air_canvas/test_air_canvas.py
```
